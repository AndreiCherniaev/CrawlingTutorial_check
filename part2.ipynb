{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1597198427785",
   "display_name": "Python 3.7.4 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's use what we have learned in part1 and apply it in a real project.\n",
    "### Here we will be downloading some images from a website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "import shutil\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load page and get html\n",
    "url=\"https://wall.alphacoders.com/by_collection.php?id=620&page=1\"\n",
    "req = urllib.request.Request(url)\n",
    "\n",
    "file_ = urllib.request.urlopen(req)\n",
    "\n",
    "webpage = file_.read()\n",
    "#Parse the html in the 'webpage' variable, and store it in Beautiful Soup format\n",
    "soup = BeautifulSoup(webpage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "https://images8.alphacoders.com/533/533007.png\nhttps://images5.alphacoders.com/314/314574.png\nhttps://images7.alphacoders.com/599/599379.jpg\nhttps://images8.alphacoders.com/632/632051.png\nhttps://images4.alphacoders.com/640/640956.jpg\nhttps://images5.alphacoders.com/632/632079.png\nhttps://images4.alphacoders.com/294/294344.jpg\nhttps://images5.alphacoders.com/301/301609.jpg\nhttps://images8.alphacoders.com/632/632052.jpg\nhttps://images7.alphacoders.com/336/336739.jpg\nhttps://images7.alphacoders.com/333/333852.jpg\nhttps://images5.alphacoders.com/603/603246.jpg\nhttps://images7.alphacoders.com/426/426351.jpg\nhttps://images7.alphacoders.com/554/554406.jpg\nhttps://images5.alphacoders.com/525/525926.jpg\nhttps://images6.alphacoders.com/505/505441.jpg\nhttps://images4.alphacoders.com/632/632068.jpg\nhttps://images7.alphacoders.com/632/632918.jpg\nhttps://images7.alphacoders.com/299/299402.jpg\nhttps://images5.alphacoders.com/311/311019.jpg\nhttps://images6.alphacoders.com/632/632908.jpg\nhttps://images6.alphacoders.com/632/632048.jpg\nhttps://images3.alphacoders.com/632/632058.jpg\nhttps://images6.alphacoders.com/301/301606.jpg\nhttps://images5.alphacoders.com/310/310747.jpg\nhttps://images6.alphacoders.com/312/312919.png\nhttps://images3.alphacoders.com/632/632075.jpg\nhttps://images6.alphacoders.com/317/317930.jpg\nhttps://images5.alphacoders.com/299/299435.jpg\nhttps://images8.alphacoders.com/374/374846.jpg\n"
    }
   ],
   "source": [
    "# extract urls\n",
    "urls = soup.find_all(class_=\"boxgrid\")\n",
    "for url in urls:\n",
    "    print(url.img['data-src'].replace('thumb-350-', ''))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now let's iterate through all the pages and get all the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some websites ask for a user agent returning 403 error. You can find it by typing \"what is my user agent in browser\"\n",
    "userAgent = {'User-Agent': 'Mozilla/5.0 (Windows NT 6.3; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/75.0.3770.142 Safari/537.36'}\n",
    "\n",
    "for i in range(1, 146):\n",
    "\n",
    "    url = \"https://wall.alphacoders.com/by_collection.php?id=620&page=\" + str(i)\n",
    "    req = urllib.request.Request(url, headers=userAgent)\n",
    "\n",
    "    file_ = urllib.request.urlopen(req)\n",
    "\n",
    "    webpage = file_.read()\n",
    "\n",
    "    webpage = webpage.decode('utf-8')\n",
    "\n",
    "    soup = BeautifulSoup(webpage)\n",
    "    cards = soup.find_all(class_=\"boxgrid\")\n",
    "\n",
    "    for card in cards:\n",
    "        url = card.img['data-src'].replace('thumb-350-', '')\n",
    "        res = requests.get(url, stream=True, headers=userAgent)\n",
    "        # if everything is ok\n",
    "        if res.status_code == 200:\n",
    "            with open('./data/img/' + url[-10:], 'wb') as imageFile:\n",
    "                res.raw.decode_content = True\n",
    "                shutil.copyfileobj(res.raw, imageFile)\n",
    "                imageFile.close()\n",
    "        print(url)"
   ]
  }
 ]
}