{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1597206153857",
   "display_name": "Python 3.7.4 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below works well with server-side compiled pages. However, it fails with web pages that are rendered on the client-side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<p class=\"jstest\" id=\"yesnojs\">y u bad tho?</p>"
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "import urllib\n",
    "#import the Beautiful soup functions to parse the data returned from the website\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "link = \"https://pythonprogramming.net/parsememcparseface/\"\n",
    "f = urllib.request.urlopen(link)\n",
    "myfile = f.read()\n",
    "soup = BeautifulSoup(myfile)\n",
    "soup.find(class_='jstest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<p class=\"jstest\" id=\"yesnojs\">y u bad tho?</p>"
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "import urllib\n",
    "#import the Beautiful soup functions to parse the data returned from the website\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://pythonprogramming.net/parsememcparseface/\"\n",
    "userAgent = {'User-Agent': 'Mozilla/5.0 (Windows NT 6.3; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/75.0.3770.142 Safari/537.36'}\n",
    "req = urllib.request.Request(url, headers=userAgent)\n",
    "\n",
    "file_ = urllib.request.urlopen(req)\n",
    "\n",
    "webpage = file_.read()\n",
    "\n",
    "#Parse the html in the 'page' variable, and store it in Beautiful Soup format\n",
    "soup = BeautifulSoup(webpage)\n",
    "soup.find(class_='jstest')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Then is there a solution for those pages as well?\n",
    "- There is! You need to use a Web Browser.\n",
    "- Which?\n",
    "- Any! For example, PyQt or Selenium for Python, or Phantomjs for JavaScript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Load finished\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<p class=\"jstest\" id=\"yesnojs\">Look at you shinin!</p>"
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "import bs4 as bs\n",
    "from PyQt5 import QtCore, QtWidgets, QtWebEngineWidgets\n",
    "\n",
    "class WebPage(QtWebEngineWidgets.QWebEnginePage):\n",
    "    def __init__(self):\n",
    "        self.app = QtWidgets.QApplication(sys.argv)\n",
    "        super(WebPage, self).__init__()\n",
    "        # connect hadler that will be executed when page load is complete\n",
    "        self.loadFinished.connect(self.handleLoadFinished)\n",
    "        self.html = ''\n",
    "\n",
    "    def start(self, url):\n",
    "        self.load(QtCore.QUrl(url))\n",
    "        # wait till load is complete\n",
    "        self.app.exec_()\n",
    "\n",
    "    def processCurrentPage(self, html_str):\n",
    "        # store content of the page as html\n",
    "        self.html = html_str\n",
    "        QtWidgets.qApp.quit()\n",
    "\n",
    "    def handleLoadFinished(self):\n",
    "        self.toHtml(self.processCurrentPage)\n",
    "        print('Load finished')\n",
    "\n",
    "# create instance of a Web Engine\n",
    "webpage = WebPage()\n",
    "\n",
    "webpage.start('https://pythonprogramming.net/parsememcparseface/')\n",
    "\n",
    "soup = bs.BeautifulSoup(webpage.html, 'html.parser')\n",
    "soup.find(class_='jstest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import bs4 as bs\n",
    "from PyQt5 import QtCore, QtWidgets, QtWebEngineWidgets\n",
    "\n",
    "class WebPage(QtWebEngineWidgets.QWebEnginePage):\n",
    "    def __init__(self):\n",
    "        self.app = QtWidgets.QApplication(sys.argv)\n",
    "        super(WebPage, self).__init__()\n",
    "        # connect hadler that will be executed when page load is complete\n",
    "        self.loadFinished.connect(self.handleLoadFinished)\n",
    "        self.html = ''\n",
    "\n",
    "    def start(self, url):\n",
    "        self.load(QtCore.QUrl(url))\n",
    "        # wait till load is complete\n",
    "        self.app.exec_()\n",
    "\n",
    "    def processCurrentPage(self, html_str):\n",
    "        # store content of the page as html\n",
    "        self.html = html_str\n",
    "        QtWidgets.qApp.quit()\n",
    "\n",
    "    def handleLoadFinished(self):\n",
    "        self.toHtml(self.processCurrentPage)\n",
    "        print('Load finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create instance of a Web Engine\n",
    "webpage = WebPage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Load finished\n"
    }
   ],
   "source": [
    "# open webpage\n",
    "webpage.start('https://www.investing.com/news/forex-news')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# parse webpage and extract all urls\n",
    "soup = bs.BeautifulSoup(webpage.html, 'html.parser')\n",
    "largeTitle = soup.find('div', class_='largeTitle')\n",
    "articles = largeTitle.find_all('article')\n",
    "urls = []\n",
    "for art in articles:\n",
    "    a_tag = art.find('a')\n",
    "    if a_tag and a_tag['href'].startswith('/news/'):\n",
    "        urls.append(a_tag['href'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Load finished\nLoad finished\nLoad finished\nLoad finished\nLoad finished\nLoad finished\nLoad finished\nLoad finished\nLoad finished\nLoad finished\n"
    }
   ],
   "source": [
    "# open each url and save data to a file\n",
    "for url in urls:\n",
    "    # load page and get html\n",
    "    webpage.start('https://www.investing.com' + str(url))\n",
    "    soup = bs.BeautifulSoup(webpage.html, 'html.parser')\n",
    "    header = soup.find(class_ = 'articleHeader').text\n",
    "    originUrl = soup.find(class_ = 'contentSectionDetails').find('a')['href']\n",
    "    postDate =soup.find(class_ = 'contentSectionDetails').find('span').text\n",
    "    imageUrl = soup.find(class_ = 'WYSIWYG articlePage').find(id = 'carouselImage')['src']\n",
    "    text = soup.find(class_ = 'WYSIWYG articlePage').text\n",
    "    json = {'header':header, 'originUrl':originUrl, 'postDate':postDate, 'imageUrl':imageUrl, 'text':text}\n",
    "    f = open('investing.txt', 'a+')\n",
    "    f.write(str(json) + '/n')\n",
    "    f.close()"
   ]
  }
 ]
}